arXiv:2003.00744v1 [cs.CL] 2Mar 2020

PhoBERT: Pre-trained language models for Vietnamese

Dat Quoc Nguyen and Anh "Dian Nguyen
VinAI Research, Vietnam
{v.datnq9, v.anhnt496}@vinai.io

Abstract

We present PhoBERT with two versions of “base"
and “large”—the ﬁrst public large-scale monolin-
gual language models pre-trained for Vieuiamese.
We show that PhoBERT improves the state-of-
the-art in multiple Vietnamese-speciﬁc NLP tasks
including Part-of-speech tagging. Named-entity
recognition and Natural language inference. We
release PhoBERT to facilitate future research and
downstream applications for Vietnamese NLP. Our
PhoBERT is released at: ht tps : //git hub.
com/VinAIResearch/PhoBERT.

1 Introduction

Pre-trained language models. especially BERT—the Bidirec-
tional Encoder Representations from Transformers lDevlin
et al.. 20191. have recently become extremely popular and
helped to produce significant improvement gains for various
NLP tasks. The success of pre-trained BERT and its variants
has largely been limited to the English language. For other
languages. one could retrain a language-speciﬁc model using
the BERT architecture 1Vu et al.. 2019; Martin et al.. 2019;
de Vries et al.. 20191 or employ existing pre-trained multi-

lingual BERT-based models lDevlin et al.. 2019; Conneau et
al.. 2019; Conneau and Lample. 2019].

In terms of Vietnamese language modeling. to the best of
our knowledge. there are two main concerns: (i) The Viet-
namese Wikipedia corpus is the only data used to train all
monolingual language models Wu et al.. 20191. and it also
is the only Vietnamese dataset included in the pre-training
data used by all multilingual language models except XLM-R
lConneau et al.. 20191. It is worth noting that Wikipedia data
is not representative of a general language use. and the Viet-
namese Wikipedia data is relatively small (1GB in size un-
compressed). while pre-trained language models can be sig-
niﬁcantly improved by using more data lLiu et al.. 20191.
(ii) All monolingual and multilingual models. except ETNLP
lVu et al.. 20191. are not aware of the difference between
Vietnamese syllables and word tokens (this ambiguity comes
from the fact that the white space is also used to separate
syllables that constitute words when written in Vietnamese).
Without doing a pre-process step of Vietnamese word seg-
mentation. those models directly apply Bype-Pair encoding
(BPE) methods lSennrich et al.. 20161 to the syllable-level
pre-training Vietnamese data. Also. although performing

word segmentation before applying BPE on the Vietnamese
Wikipedia corpus. ETNLP in fact does not publicly release
any pre-trained BERT-based model.‘ As a result. we ﬁnd dif-
ficulties in applying existing pre-trained language models for
word-level Vietnamese NLP tasks.

To handle the two concerns above. we train the ﬁrst large-
scale monolingual BERT-based “base" and “large" models
using a 20GB word-level Vietnamese corpus. We evaluate
our models on three downstream Vietnamese NLP tasks: the
two most common ones of Part-of-speech (POS) tagging and
Named-entity recognition (NER). and a language understand-
ing task of Natural language inference (NLI). Experimental
results show that our models obtain state-of-the-art (SOTA)
performances for all three tasks. We release our models under
the name PhoBERT in popular open-source libraries. hoping
that PhoBERT can serve as a strong baseline for future Viet-
namese NLP research and applications.

2 PhoBERT

This section outlines the architecture and describes the pre-
training data and optimization setup we use for PhoBERT.
Architecture: PhoBERT has two versions PhoBERTb.m. and
Ph0BERT|u[g¢. using the same conﬁguration as BERTM... and
BERT|u[gc. respectively. PhoBERT pre-training approach is
based on RoBERTa [Liu et al.. 20191 which optimizes the
BERT pre-training method for more robust performance.
Data: We use a pre-training dataset of 20GB of uncom-
pressed texts after cleaning. This dataset is a combination of
two corpora: (i) the first one is the Vietnamese Wikipedia cor-
pus (~1GB). and (ii) the second corpus (~19GB) is a subset
of a 40GB Vieuiamese news corpus after ﬁltering out similar
news and duplications} We employ RDRSegmenter lNguyen
et al.. 20181 from VnCoreNLP lVu et al.. 20181 to perform
word and sentence segmentation on the pre-training dataset.
resulting in ~145M word-segmented sentences (~3B word
tokens). Different from RoBERTa. we then apply fastBPE
1Sennrich et al.. 20161 to segment these sentences with sub-
word units. using a vocabulary size of 64K subword types.
Optimization: We employ the RoBERTa implementation in
fairseq lOtt et al.. 2019]. Each sentence contains at most
256 subword tokens (here. 5K/145M sentences with more

lhttps : //git hub . com/vietnlp/etnlp — last access
on the 28th February 2020.

zhttpsz//github.com/binhvq/news-corpus.
crawled from a wide range of websites with 14 different topics.

Table 1: Performance scores (in ‘/c) on test sets. "Acc." abbreviates accuracy. [$1. 1*]. [O] and [Q] denote results reported by Nguyen er al.
(2017). Nguyen (2019). Vu et al. (2018) and Vu et al. (2019). respectively. "mBiLSTM" denotes a BiLSTM-based multilingual embedding
method. Note that there are higher NLI results reported for XLM-R when line-tuning on the concatenation of all 15 training datasets in the
XNLI corpus. However. those results are not comparable as we only use the Vietnamese monolingual training data for ﬁne-tuning.

 

pus tagging NER NLI
Model Model Acc.
RDRPOSTagger lNguyen et al.. 201-lllil BiLSTM-CNN-CRF  mBiLSTMlAr1etxe and Schwenk. 20191 72.0
BiLS1M-CNN-CRFlMa and Hovy. 20161 [$1 VnCoreNLP-NER lVu etaI.. 20181 multilingual BERT lWu and Dredze. 20191 69.5
VnCoreNLP-POS lNguyen et al.. 20171 VNER lNguyen et al.. 20l9bl XLMMUWTLM lConneau and Lample. 20191 76.6
jP’TDP-v2 lNguyen and Verspoor. 2018] [*1 BiLSTM-CNN-CRF + ETNLP [Q] XLM-Rh“ [Conneau et al.. 2019] 75.4
jointWPD lNguyen. 2019] VnCoreNLP-NER + ETNLP  XLM-Rum: [Conneau 0! al.. 2019] 

 

PhoBERTr....
PhoBERT.m,

than 256 subword tokens are skipped). Following Liu et
al. 12019]. we optimize the models using Adam [Kingma and
Ba. 2014]. We use a batch size of 1024 and a peak learn-
ing rate of 0.0004 for PhoBERTbN. and a batch size of 512
and a peak learning rate of 0.0002 for PhoBERTr_,,gc. We
run for 40 epochs (here. the learning rate is warmed up for
2 epochs). We use 4 Nvidia V 100 GPUs (16GB each). result-
ing in about 540K training steps for PhoBERTbN and 1.08M
steps for PhoBERT.._,,ge. We pretrain PhoBERTbN during 3
weeks. and then PhoBERT.u,g¢ during 5 weeks.

3 Experiments

We evaluate the performance of PhoBERT on three down-
stream Vietnamese NLP tasks: POS tagging. NER and NL1.

Experimental setup: For the two most common Vietnamese
POS tagging and NER tasks. we follow the VnCoreNLP setup
lVu et al.. 2018]. using standard benchmarks of the VLSP
2013 POS tagging dataset and the VLSP 2016 NER dataset
lNguyen et al.. 2019al. For NL1. we use the Vietnamese val-
idation and test sets from the XNLI corpus v1.0 [Conneau
et al.. 2018] where the Vietnamese training data is machine-
translated from English. Unlike the 2013 POS tagging and
2016 NER datasets which provide the gold word segmenta-
tion. for NL1. we use RDRSegmenter to segment the text into

words before applying f astBPE to produce subwords from
word tokens.

Following Devlin et al. 120191. for POS tagging and NER.

we append a linear prediction layer on top of the PhoBERT
architecture w.r.t. the ﬁrst subword token of each word. We
ﬁne-tune PhoBERT for each task and each dataset indepen-
dently. employing the Hugging Face transformers for
POS tagging and NER and the RoBERTa implementation in
fairseq for NLI. We use AdamW lLoshchilov and Hutter.
20191 with a ﬁxed learning rate of l.e-5 and a batch size of
32. We ﬁne-tune in 30 training epochs. evaluate the task per-
formance after each epoch on the validation set (here. early
stopping is applied when there is no improvement after 5 con-
tinuous epochs). and then select the best model to report the
ﬁnal result on the test set.
Main results: Table 1 compares our PhoBERT scores with
the previous highest reported results. using the same exper-
imental setup. PhoBERT helps produce new SOTA results
for all the three tasks. where unsurprisingly PhoBERT..,,g.. ob-
tains higher performances than PhoBERTbN.

For POS tagging. PhoBERT obtains about 0.8% abso-
lute higher accuracy than the feature- and neural network-

References

lArtetxe and Schwenk. 20191 Mikel Artetxe and Holger

Schwenk. Massively multilingual sentence embeddings
for zero-shot cross-lingual transfer and beyond. TACL.
7:597-610. 2019.

lConneau and Lample. 20l9l Alexis Conneau and Guil-
laume Lample. Cross-lingual language model pretraining.
In Proceedings of NeurlPS . pages 7059-7069. 2019.

lConneau etaI..2018l Alexis Conneau. Ruty Rinott. Guil-
laume Lample. Holger Schwenk. Ves Stoyanov. Adina
Williams. and Samuel R. Bowman. XNLI: Evaluating

cross-lingual sentence representations. In Proceedings of
EMNLP. pages 2475-2485. 2018.

[Conneau etal.. 2019] Alexis Conneau. Kartikay Khandel-
wal. Naman Goyal. Vishrav Chaudhary. Guillaume Wen-
zek. Francisco Guzman. Edouard Grave. Myle Ott. Luke
Zettlemoyer. and Veselin Stoyanov. Unsupervised cross-
lingual representation learning at scale. arXiv preprint.
arXiv:l91l.02116. 2019.

lde Vries et al.. 20191 VVietse de Vries. Andreas van Cranen-
burgh. Arianna Bisazza. Tommaso Caselli. Gertjan van
Noord. and Malvina Nissim. BERTje: A Dutch BERT
Model. arXivpreprint. arXiv: l9l2.09582. 2019.

lDev1in etal.. 20191 Jacob Devlin. Ming-Wei Chang. Ken-
ton Lee. and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understand-
ing. In Proceedings ofNAACL. pages 4171-4186. 2019.

lKingma and Ba. 20141 Diederik P. Kingma and Jimmy Ba.
Adam: A Method for Stochastic Optimization. arXiv
preprint. arXiv:1412.6980. 2014.

lLiu etaI..2019l Yinhan Liu. Myle Ott. Naman Goyal.
Jingfei Du. Mandar Joshi. Danqi Cheri. Omer Levy.
Mike Lewis. Luke Zettlemoyer. and Veselin Stoyanov.
RoBERTa: A Robustly Optimized BERT Pretraining Ap-
proach. arXiv preprint. arXiv:1907.l1692. 2019.

lLoshchilov and Hutter. 20191 llya Loshchilov and Frank
Hutter. Decoupled weight decay regularization. In Pro-
ceedings ofICLR. 2019.

lMa and I-lovy. 20161 Xuezhe Ma and Eduard Hovy. End-
to-end sequence labeling via bi-directional LSTM-CNNs-
CRF. In Proceedings ofACL. pages 1064-1074. 2016.

lMar1in etal..20l9l Louis Martin. Benjamin Muller. Pe-
dro Javier Ortiz Suarez. Yoann Dupont. Laurent Ro-
mary. Eric Villemonte de la Clergerie. Djamé Seddah. and

Benoit Sagot. CamemBERT: a Tasty French Language
Model. arXiv preprint. arXiv: 191 1.03894. 2019.

[Nguyen and Verspoor. 20l8l Dat Quoc Nguyen and Karin
Verspoor. An improved neural network model for joint
POS tagging and dependency parsing. In Proceedings of
the C oNLL 2018 Shared Task. pages 81-91. 2018.

lNguyen et al.. 20141 Dat Quoc Nguyen. Dai Quoc Nguyen.
Dang Duc Pham. and Son Bao Pham. RDRPOSTagger:
A Ripple Down Rules-based Part-Of-Speech Tagger. In
Proceedings of the Demonstrations at EAC L. pages 17-20.
2014.

cm PhoBER'1“.,_..¢ 93_.o PhoBERTr,,,.. 78.5
96.8 PhoBERT,_.,L.c 94.7 PhoBERT.m.c 30.0

based models VnCoreNLP-POS (i.e. VnMarMoT) and join-
tWPD. For E. PhoBERT|u,gc is 1.1 points higher F1 than
PhoBERT1,._,,.. which is 2+ points higher than the feature-
and neural network-based models VnCoreNLP-NER and
BiLSTM-CNN-CRF trained with the BERT-based ETNLP
word embeddings lVu et al.. 20191. For LU. PhoBERT out-

performs the multilingual BERT and the BERT-based cross-
lingual model with a new translation language modeling ob-

jective XLMMLVMLM by large margins. PhoBERT also per-

forms slightly better than the cross-lingual model XLM-R.
but using far fewer parameters than XLM-R (base: 135M vs.
250M; large: 370M vs. 560M).

Discussion: Using more pre-training data can help signiﬁ-
cantly improve the quality of the pre-trained language mod-
els lLiu et al.. 20191. Thus it is not surprising that PhoBERT
helps produce better performance than ETNLP on NER. and
the multilingual BERT and XLMMLM.,1LM on NLI (here.
PhoBERT employs 20GB of Vietnamese texts while those
models employ the 1GB Vietnamese Wikipedia data).

Our PhoBERT also does better than XLM-R which uses a
2.5TB pre-training corpus containing 137GB of Vietnamese
texts (i.e. about 137/20 z 7 times bigger than our pre-
training corpus). Recall that PhoBERT performs segmenta-
tion into subword units after performing a Vietnamese word
segmentation. while XLM-R directly applies a BPE method
to the syllable-level pre-training Vietnamese data. Clearly.
word-level information plays a crucial role for the Viet-
namese language understanding task of NL1. i.e. word seg-
mentation is necessary to improve the NLI performance. This
reconﬁrms that dedicated language-speciﬁc models still out-
perform multilingual ones [Martin etal.. 2019].

Experiments also show that using a straightforward ﬁne-
tuning manner as we do can lead to SOTA results. Note that
we might boost our downstream task performances even fur-
ther by doing a more careful hyper-parameter ﬁne-tuning.

4 Conclusion

In this paper. we have presented the ﬁrst public large-scale
PhoBERT language models for Vietnamese. We demonstrate
the usefulness of PhoBERT by producing new state-of-the-
art performances for three Vietnamese NLP tasks of POS
tagging. NER and NL1. By publicly releasing PhoBERT. we
hope that it can foster future research and applications in Viet-
namse NLP. Our PhoBERT and its usage are available at:
https://github.com/VinAIResearch/PhoBERT.

lNguyen etaI.. 20l7l Dat Quoc Nguyen. Thanh Vu.
Dai Quoc Nguyen. Mark Dras. and Mark Johnson. From
word segmentation to POS tagging for Vietnamese. In
Proceedings ofALTA. pages 108-1 13. 2017.

[Nguyen et al.. 2018] Dat Quoc Nguyen. Dai Quoc Nguyen.
Thanh Vu. Mark Dras. and Mark Johnson. A Fast and
Accurate Vietnamese Word Segmenter. In Proceedings of
LREC. pages 2582-2587. 2018.

lNguyen etaI.. 2019al I-Iuyen Nguyen. Quyen Ngo. Luong
Vu. Vu Tran. and Hien Nguyen. VLSP Shared Task:
Named Entity Recognition. Journal of Computer Science
and Cybemetics. 34(4):283—294. 2019.

lNguyen etaI.. 20l9bl Kim Anh Nguyen. Ngan Dong. and
Cam—Tu Nguyen. Attentive neural network for named en-
tity recognition in vietnamese. In Proceedings of RIVF.
2019.

lNguyen. 20191 Dat Quoc Nguyen. A neural joint model
for Vietnamese word segmentation. POS tagging and de-
pendency parsing. In Proceedings of ALTA. pages 28-34.
2019.

lOttetaI..2019l Myle Ott. Sergey Edunov. Alexei Baevski.
Angela Fan. Sam Gross. Nathan Ng. David Grangier. and
Michael Auli. fairseq: A fast. extensible toolkit for se-
quence modeling. In Proceedings of NAACL-HLT 2019:
Demonstrations. 201 9.

lSennrich etal.. 20161 Rico Sennrich. Barry I-laddow. and
Alexandra Birch. Neural machine translation of rare words

with subword units. In Proceedings ofACL. pages 1715-
1725. 2016.

lVu eta1.. 20l8l Thanh Vu. Dat Quoc Nguyen. Dai Quoc
Nguyen. Mark Dras. and Mark Johnson. VnCoreNLP: A
Vietnamese Natural Language Processing Toolkit. In Pro-
ceedings of NAAC L: Demonstrations. pages 56-60. 2018.

lVu et al.. 20191 Xuan-Son Vu. Thanh Vu. Son Tran. and Lili
Jiang. ETNLP: A visual-aided systematic approach to se-
lect pre-trained embeddings for a downstream task. In Pro-
ceedings ofRANLP. pages 1285-1294. 2019.

lWu and Dredze. 20l9l Shijie Wu and Mark Dredze. Beto.
bentz. becas: The surprising cross-lingual effectiveness of
BERT. In Proceedings of EMNLP-IJCNLP. pages 833-
844. 2019.

